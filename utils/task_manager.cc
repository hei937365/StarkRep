#include "starkware/utils/task_manager.h"

#include <algorithm>
#include <utility>

#include "glog/logging.h"

#include "starkware/math/math.h"

DEFINE_uint32(n_threads, std::thread::hardware_concurrency(), "Number of threads to use.");

namespace starkware {

TaskManager::TaskManager(const size_t n_threads) {
  ASSERT_RELEASE(n_threads > 0, "Number of threads must be at least 1.");

  SetWorkerIdForCurrentThread(0);
  for (size_t i = 0; i < n_threads - 1; i++) {
    workers_.emplace_back([this, id = i + 1]() {
      SetWorkerIdForCurrentThread(id);
      TaskRunner(&new_pending_task_, &continue_running_);
    });
  }
}

TaskManager::~TaskManager() {
  {
    std::unique_lock<std::mutex> cv_lock(mutex_);
    // Log rather than assert, because we don't want to throw exceptions in a destructor.
    LOG_IF(ERROR, !tasks_.empty()) << "Threadpool destructor called while tasks are pending.";
    continue_running_ = 0;
    new_pending_task_.NotifyAll();
    task_group_finished_.NotifyAll();
  }
  for (auto& t : workers_) {
    t.join();
  }
}
void TaskManager::InitSingleton() { singleton = new TaskManager(FLAGS_n_threads); }

TaskManager TaskManager::CreateInstanceForTesting(size_t n_threads) {
  return TaskManager(n_threads);
}

void TaskManager::TaskRunner(CvWithWaitersCount* cv, const size_t* siblings_counter) {
  for (;;) {
    std::unique_lock<std::mutex> lock(mutex_);
    while (*siblings_counter > 0 && tasks_.empty()) {
      cv->Wait(&lock);
    }

    if (*siblings_counter == 0) {
      // There are two ways of reaching this point:
      // 1. The destructor was called and siblings_counter == &continue_running_.
      // 2. We are in the context of a ParallelFor call and all the tasks generated by
      // that call have been completed.
      // Note that in this case there may still be tasks from a separate ParallelFor invocation,
      // so tasks_ might not be empty.

      return;
    }

    const auto task = std::move(tasks_.back());
    tasks_.pop_back();
    if (!tasks_.empty()) {
      // There are two events where would like to wake up a sleeping thread:
      // 1. There is a new task waiting to be executed.
      // 2. The group of tasks that the thread is waiting for have been completed.
      // Ideally we would have liked a thread to wait for both events, but there is no
      // wait for multiple events in the standard library.
      // As a workaround, threads sleep on either event 1 or event 2 and when we have a pending task
      // we check both events for waiting threads.
      if (!new_pending_task_.TryNotify()) {
        task_group_finished_.TryNotify();
      }
    }
    lock.unlock();
    task();
  }
}

/*
  Returns the equivelent of std::min(start_idx + chunk_size, end_idx) while taking care of
  uint64_t overflow in start_idx + chunk_size.
*/
static uint64_t GetTaskEndIdx(uint64_t start_idx, uint64_t chunk_size, uint64_t end_idx) {
  uint64_t res = start_idx + chunk_size;
  if (res < start_idx || res > end_idx) {
    res = end_idx;
  }
  return res;
}

void TaskManager::ParallelFor(
    uint64_t start_idx, uint64_t end_idx, const std::function<void(const TaskInfo&)>& func,
    uint64_t max_chunk_size_for_lambda, uint64_t min_work_chunk) {
  uint64_t split_size = std::max(
      min_work_chunk, DivCeil(end_idx - start_idx, kTaskRedundancyFactor * GetNumThreads()));

  size_t siblings_counter = 0;
  std::exception_ptr eptr = nullptr;

  std::unique_lock<std::mutex> cv_lock(mutex_);

  for (uint64_t task_end_idx, task_idx = start_idx; task_idx < end_idx; task_idx = task_end_idx) {
    ++siblings_counter;

    task_end_idx = GetTaskEndIdx(task_idx, split_size, end_idx);

    uint64_t chunk_size = max_chunk_size_for_lambda;
    tasks_.emplace_back(
        [this, &func, task_idx, task_end_idx, chunk_size, &siblings_counter, &eptr] {
          std::exception_ptr exception = nullptr;
          struct TaskInfo info {};
          uint64_t i;
          for (i = task_idx; i < task_end_idx; i = info.end_idx) {
            info.start_idx = i;
            info.end_idx = GetTaskEndIdx(i, chunk_size, task_end_idx);

            try {
              func(info);
            } catch (...) {
              exception = std::current_exception();
              break;
            }
          }

          std::unique_lock<std::mutex> lock(mutex_);
          if (exception != nullptr && eptr == nullptr) {
            eptr = exception;
          }

          --siblings_counter;
          if (siblings_counter == 0) {
            task_group_finished_.NotifyAll();
          }
        });
  }

  cv_lock.unlock();
  TaskRunner(&task_group_finished_, &siblings_counter);

  // All the tasks that were spawned leading up to this point should have been completed.
  // Therefore it is safe to read eptr without a lock.
  ASSERT_RELEASE(siblings_counter == 0, "TaskRunner returned before all siblings completed.");

  if (eptr != nullptr) {
    std::rethrow_exception(eptr);
  }
}

gsl::owner<TaskManager*> TaskManager::singleton;
std::once_flag TaskManager::singleton_flag;
thread_local size_t TaskManager::worker_id;

}  // namespace starkware
